<!DOCTYPE html><html lang="en">
<head>
<title>DefReliability &lt; CTSpedia &lt; Foswiki</title>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.5, user-scalable=yes" />
<meta name="mobile-web-app-capable" content="yes" />
<meta name="mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<link href="../MISSING RESOURCE CTSpedia/DefReliability/favicon.ico" rel="icon" type="image/x-icon"> <link rel="shortcut icon" href="../MISSING RESOURCE CTSpedia/DefReliability/favicon.ico" type="image/x-icon">
<link href="DefReliability.html" rel="alternate" type="application/x-wiki" title="edit DefReliability">
<meta name="description" content="DefReliability" />
<link type="text/css" href="../System/SmiliesPlugin.attachments/smilies.css" class="head SMILIESPLUGIN" media="all" rel="stylesheet"><!--SMILIESPLUGIN-->
<script class="script JQUERYPLUGIN" src="../System/JQueryPlugin.attachments/jquery-2.2.4.js"></script><!--JQUERYPLUGIN-->
<script class="script JQUERYPLUGIN::OBSERVER" src="../System/JQueryPlugin.attachments/plugins/observer/observer.js?version=0.1"></script><!--JQUERYPLUGIN::OBSERVER-->
<script src="../System/JQueryPlugin.attachments/plugins/migrate/jquery.migrate.js?version=3.3.1" class="script JQUERYPLUGIN::MIGRATE"></script><!--JQUERYPLUGIN::MIGRATE-->
<script src="../System/JQueryPlugin.attachments/plugins/foswiki/jquery.foswiki.js?version=3.01" class="script JQUERYPLUGIN::FOSWIKI"></script><!--JQUERYPLUGIN::FOSWIKI-->
<script class="script JQUERYPLUGIN::BROWSER" src="../System/JQueryPlugin.attachments/plugins/browser/jquery.browser.js?version=0.1.0"></script><!--JQUERYPLUGIN::BROWSER-->
<script class='script JQUERYPLUGIN::FOSWIKI::PREFERENCES foswikiPreferences' type='text/json'>{
   "TOPIC" : "DefReliability",
   "SERVERTIME" : "08 Feb 2023 - 15:50",
   "WIKIUSERNAME" : "Main.WikiAdmin",
   "USERSWEB" : "Main",
   "NAMEFILTER" : "[\\\\\\s*?~^$@%`\"'&|<:;>\\[\\]#\\x00-\\x1f]",
   "PUBURLPATH" : "/foswiki/pub",
   "USERNAME" : "WikiAdmin",
   "WEB" : "CTSpedia",
   "SCRIPTURLPATHS" : {},
   "PUBURL" : "http://biostat1478.dhcp.mc.vanderbilt.edu/foswiki/pub",
   "COOKIEREALM" : "",
   "SCRIPTURL" : "http://biostat1478.dhcp.mc.vanderbilt.edu/foswiki/bin",
   "SYSTEMWEB" : "System",
   "URLHOST" : "http://biostat1478.dhcp.mc.vanderbilt.edu",
   "SKIN" : "natedit,pattern",
   "SCRIPTURLPATH" : "/foswiki/bin",
   "SCRIPTSUFFIX" : "",
   "WIKINAME" : "WikiAdmin"
}
</script><!--JQUERYPLUGIN::FOSWIKI::PREFERENCES-->
</head>
<body>
<h1 id="Reliability">  Reliability </h1>
 <strong><em>Lead Author(s):</em></strong> Erin Esp 
<div class="foswikiToc" id="foswikiTOC"> <ul>
<li> <a href="#Reliability"> Reliability </a> <ul>
<li> <a href="#Definition"> Definition </a>
</li> <li> <a href="#Types_of_Estimates"> Types of Estimates </a> <ul>
<li> <a href="#Test_45Retest_Reliability"> Test-Retest Reliability </a>
</li> <li> <a href="#Internal_Consistency_Reliability"> Internal Consistency Reliability </a>
</li> <li> <a href="#Inter_45Rater_Reliability"> Inter-Rater Reliability </a>
</li> <li> <a href="#Inter_45Method_Reliability"> Inter-Method Reliability </a>
</li></ul> 
</li></ul> 
</li></ul> 
</div> 
<h2 id="Definition">  Definition </h2>
 Reliability is the consistency of a set of measurements or of a measuring instrument. In short, reliability is the repeatability of a measurement. Reliability is often used to describe a test. 
<h2 id="Types_of_Estimates">  Types of Estimates </h2>
 There are several different types of reliability estimates  <ol>
<li> Test-Retest Reliability
</li> <li> Internal Consistency Reliability
</li> <li> Inter-Rater Reliability
</li> <li> Inter-Method Reliability
</li></ol> 
<h3 id="Test_45Retest_Reliability">  Test-Retest Reliability </h3>
 Test-Retest reliability is the variation in measurements taken by a single person or instrument on the same item and under the same conditions. This measure is desirable mainly for measurements that are not expected to change over time.
<p></p>
One particular type of test-retest reliability is intra-rater reliability. Intra-rater reliability measures the degree of agreement among multiple repetitions of a diagnostic test performed by a single rater.
<h3 id="Internal_Consistency_Reliability">  Internal Consistency Reliability </h3>
 Internal consitency reliability assesses the consistency of results across items within a test. That is, it measures whether several items that propose to measure the same general idea produce similar scores.
<p></p>
Often times Cronbach's Alpha is used to measure this reliability. Cronbach's Alpha is a statistic calculated from the pairwise correlations between items. An alpha value between 0.6 and 0.8 indicates an acceptable reliability while alpha values greater than 0.8 indicate good reliability. We should be careful though as high reliabilities may indicate that the items are entirely redundant.
<p></p>
Another statistic used to measure internal consistency reliability is the Coefficient Omega.
<h3 id="Inter_45Rater_Reliability">  Inter-Rater Reliability </h3>
 Inter-rater reliability, also known as inter-rater agreement and concordance, measures the variation in measurements when taken by different persons but with the same method or instrument. There are a number of different statistics that can be used to measure the inter-rater reliability.  <ol>
<li> <a href="PercentAgreement.html">Overall Percent Agreement</a><br />- This measure assumes the data is entirely nominal.<br />- It does not take into account that agreement may happen solely based on chance.<br />- The least robust measure of inter-rater reliability.
</li> <li> <a href="CohensKappa.html">Cohen's Kappa</a><br />- This measure also assumes the data is entirely nominal<br />- Measures the inter-rater reliability between two raters.<br />- This measure takes into account chance agreement.
</li> <li> <a href="FleissKappa.html">Fleiss Kappa</a><br />- This measure is similar to Cohen's Kappa however it incorporates any number of raters.<br />- Assumes the data is entirely nominal.
</li> <li> Inter-rater Correlation <br />- Measures pairwise correlation among raters using a scale that is ordered.<br />- Examples include <a href="PearsonCorrelation.html">Pearson's Correlation Coefficient</a> and <a href="SpearmanRank.html">Spearman's Rank Correlation Coefficient</a>.
</li> <li> <a href="ICC.html">Intraclass Correlation Coefficient (ICC)</a><br />- Measures the proportion of variability of an observation that is accounted for by the between-group variability.
</li> <li> <a href="ConcordanceCorrelation.html">Concordance Correlation Coefficient</a><br /> - Nearly identical to the intra-class correlation.
</li></ol> 
<h3 id="Inter_45Method_Reliability">  Inter-Method Reliability </h3>
 Inter-method reliability is the variation in measurements of the same target when taken by different methods or instruments. A commonly used type of inter-method reliability is the <a href="http://www.socialresearchmethods.net/kb/reltypes.php">parallel-forms reliability</a>.
<p></p>
-- <a class="foswikiNewLink" rel="nofollow" href="../Main/ErinEsp.html" title="Create this topic">ErinEsp</a> - 24 Jul 2010
<hr />
<p></p>
<hr />
<p></p>
</body>